{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81a29592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d54f3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Feature Extractor (Color Histogram) ==========\n",
    "def extract_features(image, bins=32, resize_dim=(64, 64)):\n",
    "    image = image.resize(resize_dim).convert('RGB') # Ensures that all images are RGB\n",
    "    image_np = np.array(image)\n",
    "    hist_r = np.histogram(image_np[:, :, 0], bins=bins, range=(0, 256))[0]\n",
    "    hist_g = np.histogram(image_np[:, :, 1], bins=bins, range=(0, 256))[0]\n",
    "    hist_b = np.histogram(image_np[:, :, 2], bins=bins, range=(0, 256))[0]\n",
    "    hist = np.concatenate([hist_r, hist_g, hist_b])\n",
    "    return hist / np.sum(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "409ed06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(image, resize_dim=(64, 64), orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    image = image.resize(resize_dim).convert('RGB')\n",
    "    gray = rgb2gray(np.array(image))\n",
    "    features = hog(gray, orientations=orientations,\n",
    "                   pixels_per_cell=pixels_per_cell,\n",
    "                   cells_per_block=cells_per_block,\n",
    "                   block_norm='L2-Hys')\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def extract_combined_features(image):\n",
    "    hog_feat = extract_hog_features(image)\n",
    "    hist_feat = extract_features(image)\n",
    "    return np.concatenate([hog_feat, hist_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25798c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Load data ===\n",
      "Train shape: (15000, 1860), Test shape: (300, 1860)\n",
      "Classes: ['library-indoor' 'museum-indoor' 'shopping_mall-indoor']\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "image_size = (64, 64)  # Resize all images to 64x64\n",
    "valid_exts = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "# Function to load images from a given folder\n",
    "def load_dataset(root_dir, extractor_fn):\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(root_dir))\n",
    "    for label in class_names:\n",
    "        label_path = os.path.join(root_dir, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        for fname in os.listdir(label_path):\n",
    "            if fname.lower().endswith(valid_exts):\n",
    "                try:\n",
    "                    img_path = os.path.join(label_path, fname)\n",
    "                    img = Image.open(img_path).convert('RGB')\n",
    "                    if extractor_fn == None:\n",
    "                        img = img.resize(image_size)\n",
    "                        img_array = np.array(img).flatten()  # Flatten to 1D vector (64*64*3)\n",
    "                        X.append(img_array)\n",
    "                        y.append(label)\n",
    "                    else:\n",
    "                        features = extractor_fn(img)\n",
    "                        X.append(features)\n",
    "                        y.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_path}: {e}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load training and testing data\n",
    "print(\"=== Load data ===\")\n",
    "func_name = extract_combined_features #extract_features  #extract_combined_features #None\n",
    "X_train, y_train = load_dataset(\"Training\", func_name)\n",
    "X_test, y_test = load_dataset(\"Test\", func_name)\n",
    "\n",
    "# Encode labels (e.g., 'library' -> 0, etc.)\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Shuffle and standardize\n",
    "X_train, y_train_enc = shuffle(X_train, y_train_enc, random_state=42)\n",
    "X_test, y_test_enc = shuffle(X_test, y_test_enc, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(f\"Classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cfce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SVM Training ---\n"
     ]
    }
   ],
   "source": [
    "# ---- SVM ---- Ignore for now\n",
    "print(\"\\n--- SVM Training ---\")\n",
    "svm_params = {'C': [1, 10], 'kernel': ['linear', 'rbf']}\n",
    "svm = GridSearchCV(SVC(probability=True), svm_params, cv=3, scoring='accuracy')\n",
    "svm.fit(X_train, y_train_enc)\n",
    "svm_preds = svm.predict(X_test)\n",
    "print(classification_report(y_test_enc, svm_preds))\n",
    "print(confusion_matrix(y_test_enc, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3506a7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SVM Training ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67       100\n",
      "           1       0.75      0.72      0.73       100\n",
      "           2       0.67      0.72      0.70       100\n",
      "\n",
      "    accuracy                           0.70       300\n",
      "   macro avg       0.70      0.70      0.70       300\n",
      "weighted avg       0.70      0.70      0.70       300\n",
      "\n",
      "[[66 11 23]\n",
      " [16 72 12]\n",
      " [15 13 72]]\n"
     ]
    }
   ],
   "source": [
    "# ---- SVM ---- individual models\n",
    "# === I need you to run this for different hyperparameters,\n",
    "## I made an excel sheet where you can keep track of the results\n",
    "## For now I think it is best to run SVM with func_name= extrac combined features (it is already there)\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict_proba\n",
    "## The link above has the model details where u can see what are your options\n",
    "### For now I believe C, kernel, and degree are the ones we can change, \n",
    "### if you think i'm missing an important hyperparameter, feel free to change it\n",
    "print(\"\\n--- SVM Training ---\")\n",
    "## The commented code below runs all combinations of models to come up with the best one but it will take forever\n",
    "## Hence, just use a single model and change the values manually\n",
    "# svm_params = {'C': [1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'degree': =[3,5,8]}\n",
    "# svm_classifier = GridSearchCV(SVC(probability=True), svm_params, cv=3, scoring='accuracy')\n",
    "svm_classifier = SVC(kernel='rbf', C=1.0, random_state=42, degree= 8, probability=True)\n",
    "svm_classifier.fit(X_train, y_train_enc)\n",
    "print('=== Testing Performance ===')\n",
    "svm_preds = svm_classifier.predict(X_test)\n",
    "print(classification_report(y_test_enc, svm_preds))\n",
    "print(confusion_matrix(y_test_enc, svm_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa788b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90      5000\n",
      "           1       0.95      0.88      0.91      5000\n",
      "           2       0.89      0.93      0.91      5000\n",
      "\n",
      "    accuracy                           0.91     15000\n",
      "   macro avg       0.91      0.91      0.91     15000\n",
      "weighted avg       0.91      0.91      0.91     15000\n",
      "\n",
      "[[4552  150  298]\n",
      " [ 294 4405  301]\n",
      " [ 240   96 4664]]\n"
     ]
    }
   ],
   "source": [
    "##=== Training Performance ===\n",
    "print('=== Training Performance ===')\n",
    "svm_preds_train = svm_classifier.predict(X_train)\n",
    "print(classification_report(y_train_enc, svm_preds_train))\n",
    "print(confusion_matrix(y_train_enc, svm_preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02e545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50} --> mean: 0.5142, std: 0.0085\n",
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100} --> mean: 0.5124, std: 0.0069\n",
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200} --> mean: 0.5115, std: 0.0076\n",
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50} --> mean: 0.5139, std: 0.0098\n",
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100} --> mean: 0.5113, std: 0.0058\n",
      "{'criterion': 'gini', 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200} --> mean: 0.5149, std: 0.0078\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50} --> mean: 0.5383, std: 0.0079\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100} --> mean: 0.5452, std: 0.0063\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200} --> mean: 0.5485, std: 0.0079\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50} --> mean: 0.5375, std: 0.0058\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100} --> mean: 0.5465, std: 0.0084\n",
      "{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200} --> mean: 0.5536, std: 0.0070\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50} --> mean: 0.5263, std: 0.0087\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100} --> mean: 0.5467, std: 0.0094\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200} --> mean: 0.5625, std: 0.0048\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'n_estimators': 50} --> mean: 0.5354, std: 0.0100\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100} --> mean: 0.5487, std: 0.0066\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200} --> mean: 0.5577, std: 0.0040\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 50} --> mean: 0.5096, std: 0.0075\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100} --> mean: 0.5115, std: 0.0069\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200} --> mean: 0.5125, std: 0.0081\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50} --> mean: 0.5090, std: 0.0090\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100} --> mean: 0.5094, std: 0.0061\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200} --> mean: 0.5137, std: 0.0061\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50} --> mean: 0.5367, std: 0.0073\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100} --> mean: 0.5468, std: 0.0036\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200} --> mean: 0.5517, std: 0.0066\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 50} --> mean: 0.5365, std: 0.0076\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100} --> mean: 0.5463, std: 0.0062\n",
      "{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200} --> mean: 0.5497, std: 0.0083\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50} --> mean: 0.5359, std: 0.0041\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100} --> mean: 0.5503, std: 0.0037\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200} --> mean: 0.5621, std: 0.0099\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'n_estimators': 50} --> mean: 0.5358, std: 0.0063\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100} --> mean: 0.5501, std: 0.0057\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200} --> mean: 0.5611, std: 0.0052\n"
     ]
    }
   ],
   "source": [
    "### IGNORE ####\n",
    "\n",
    "results = rf.cv_results_\n",
    "for mean, std, params in zip(results[\"mean_test_score\"],\n",
    "                              results[\"std_test_score\"],\n",
    "                              results[\"params\"]):\n",
    "    print(f\"{params} --> mean: {mean:.4f}, std: {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6270f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30e67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26da037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05885946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Color Histogram Only ---\n",
    "def extract_color_histogram(image, bins=32, resize_dim=(64, 64)):\n",
    "    image = image.resize(resize_dim).convert('RGB')\n",
    "    image_np = np.array(image)\n",
    "    hist_r = np.histogram(image_np[:, :, 0], bins=bins, range=(0, 256))[0]\n",
    "    hist_g = np.histogram(image_np[:, :, 1], bins=bins, range=(0, 256))[0]\n",
    "    hist_b = np.histogram(image_np[:, :, 2], bins=bins, range=(0, 256))[0]\n",
    "    hist = np.concatenate([hist_r, hist_g, hist_b])\n",
    "    return hist / np.sum(hist)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
